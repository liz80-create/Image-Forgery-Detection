{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f3fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, Activation, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa020206",
   "metadata": {},
   "source": [
    "Accessing Img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f2d2a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#opening img \u001b[39;00m\n\u001b[0;32m      2\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/3rd_Year_CWIT/FYP/sign-dataset/sample_Signature/sample_Signature/genuine/NFI-01301013.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m img_g \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img_g)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "#opening img \n",
    "img = cv2.imread(\"E:/3rd_Year_CWIT/FYP/sign-dataset/sample_Signature/sample_Signature/genuine/NFI-01301013.png\")\n",
    "img_g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY )\n",
    "plt.imshow(img_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd037fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m sift \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mSIFT_create(nfeatures\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, nOctaveLayers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, contrastThreshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.04\u001b[39m, edgeThreshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.8\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#detect keypoints and descriptors\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m keypoints , descriptors \u001b[38;5;241m=\u001b[39m sift\u001b[38;5;241m.\u001b[39mdetectAndCompute(image \u001b[38;5;241m=\u001b[39m \u001b[43mimg_g\u001b[49m, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, descriptors\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Draw the keypoints on the image\u001b[39;00m\n\u001b[0;32m      9\u001b[0m img_g_with_keypoints \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdrawKeypoints(img_g, keypoints, \u001b[38;5;28;01mNone\u001b[39;00m, flags\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mDRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_g' is not defined"
     ]
    }
   ],
   "source": [
    "#program to show image with key_points and Descriptors\n",
    "#creating sift obj\n",
    "sift = cv2.SIFT_create(nfeatures=1000, nOctaveLayers=4, contrastThreshold=0.04, edgeThreshold=15, sigma=1.8)\n",
    "\n",
    "#detect keypoints and descriptors\n",
    "keypoints , descriptors = sift.detectAndCompute(image = img_g, mask = None, descriptors= None)\n",
    "\n",
    "# Draw the keypoints on the image\n",
    "img_g_with_keypoints = cv2.drawKeypoints(img_g, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Draw the descriptors on the image\n",
    "img_g_with_descriptors = cv2.drawKeypoints(img_g, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display the images\n",
    "cv2.imshow('Image with keypoints', img_g_with_keypoints)\n",
    "cv2.imshow('Image with descriptors', img_g_with_descriptors)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be85ad9",
   "metadata": {},
   "source": [
    "# Signature Matching (SIFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d09e29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#GOAL : feature matching\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m img1 \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfinal_year_project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgenuine\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m005005_000.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m img1 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img1 , cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2GRAY) \u001b[38;5;66;03m#RGB->GRAY\u001b[39;00m\n\u001b[0;32m      4\u001b[0m img1 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img1, (\u001b[38;5;241m800\u001b[39m,\u001b[38;5;241m600\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#GOAL : feature matching\n",
    "img1 = cv2.imread(r\"D:\\final_year_project\\data\\genuine\\005005_000.png\")\n",
    "img1 = cv2.cvtColor(img1 , cv2.COLOR_RGB2GRAY) #RGB->GRAY\n",
    "img1 = cv2.resize(img1, (800,600))\n",
    "img2 = cv2.imread(r\"D:\\final_year_project\\data\\genuine\\005005_000.png\")\n",
    "img2 = cv2.cvtColor(img2 , cv2.COLOR_RGB2GRAY) #RGB->GRAY\n",
    "img2 = cv2.resize(img2, (800,600))\n",
    "#sift obj\n",
    "#sift = cv2.SIFT_create(nfeatures=1000, nOctaveLayers=5, contrastThreshold=0.10, edgeThreshold=11, sigma=1.8)\n",
    "sift = cv2.SIFT_create(nfeatures=500, nOctaveLayers=5, contrastThreshold=0.09, edgeThreshold=11, sigma=1.8)\n",
    "\n",
    "\n",
    "#detect keypoints and descriptors\n",
    "kp1 , des1 = sift.detectAndCompute(image = img1, mask = None, descriptors= None)\n",
    "kp2 , des2 = sift.detectAndCompute(image = img2, mask = None, descriptors= None)\n",
    "\n",
    "#BFMatcher with def params\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1,des2,k=3)\n",
    "\n",
    "print(f\"Length of Matches :- {len(matches)}\")\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for match_list in matches:\n",
    "    m, n = match_list[:2]  # Extract the first two matches\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good.append([m])\n",
    "        \n",
    "# Calculate the percentage of match\n",
    "percentage_match = len(good) / len(kp1) * 100\n",
    "print(f\"Percentage of match: {percentage_match:.2f}%\")\n",
    "if percentage_match < 13.5:\n",
    "    print(\"forged\")\n",
    "else:\n",
    "    print(\"orginal\")\n",
    "\n",
    "print(f\"Length of good :- {len(good)}\")\n",
    "# Draw the matches\n",
    "img_matches = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow('Matches', img_matches)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ffdc003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genuine signature\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess the images\n",
    "img1_path = r\"D:\\final_year_project\\data\\genuine\\005005_000.png\"\n",
    "img2_path = r\"D:\\final_year_project\\data\\genuine\\005005_000.png\"\n",
    "\n",
    "img1 = cv2.imread(img1_path, 0)\n",
    "img2 = cv2.imread(img2_path, 0)\n",
    "\n",
    "# Check if the images were loaded successfully\n",
    "if img1 is None or img2 is None:\n",
    "    print(\"Failed to load images.\")\n",
    "    exit()\n",
    "\n",
    "# Resize and normalize the images\n",
    "img1 = cv2.resize(img1, (800, 600))\n",
    "img2 = cv2.resize(img2, (800, 600))\n",
    "img1 = cv2.bitwise_not(img1)\n",
    "img2 = cv2.bitwise_not(img2)\n",
    "\n",
    "# Extract SIFT features\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# Extract geometric features\n",
    "def extract_geometric_features(image):\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours) == 0:\n",
    "        return np.zeros(6)\n",
    "    \n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    rect = cv2.minAreaRect(c)\n",
    "    (x, y), (w, h), angle = rect\n",
    "    \n",
    "    aspect_ratio = float(w) / h\n",
    "    rect_area = w * h\n",
    "    hull_area = cv2.contourArea(c)\n",
    "    solidity = float(rect_area) / hull_area\n",
    "    extent = float(cv2.contourArea(c)) / rect_area\n",
    "    equiv_diameter = np.sqrt(4 * cv2.contourArea(c) / np.pi)\n",
    "    \n",
    "    return [aspect_ratio, solidity, extent, equiv_diameter, w, h]\n",
    "\n",
    "geom_features1 = extract_geometric_features(img1)\n",
    "geom_features2 = extract_geometric_features(img2)\n",
    "\n",
    "# Combine SIFT and geometric features\n",
    "features1 = np.concatenate((des1.flatten(), geom_features1))\n",
    "features2 = np.concatenate((des2.flatten(), geom_features2))\n",
    "\n",
    "# Normalize the features\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "features1 = scaler1.fit_transform(features1.reshape(1, -1))\n",
    "features2 = scaler2.fit_transform(features2.reshape(1, -1))\n",
    "\n",
    "# Train an SVM classifier\n",
    "X_train = np.concatenate((features1, features2), axis=0)\n",
    "y_train = np.array([1, 0])  # Assuming the first signature is genuine and the second is forged\n",
    "\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier on a new signature\n",
    "#test_img_path = r\"D:\\final_year_project\\data\\forged\\021005_002.png\"\n",
    "test_img_path = r\"D:\\final_year_project\\data\\forged\\021006_000.png\"\n",
    "test_img = cv2.imread(test_img_path, 0)\n",
    "\n",
    "# Check if the test image was loaded successfully\n",
    "if test_img is None:\n",
    "    print(\"Failed to load the test image.\")\n",
    "    exit()\n",
    "\n",
    "test_img = cv2.resize(test_img, (800, 600))\n",
    "test_img = cv2.bitwise_not(test_img)\n",
    "\n",
    "_, test_des = sift.detectAndCompute(test_img, None)\n",
    "test_geom_features = extract_geometric_features(test_img)\n",
    "test_features = np.concatenate((test_des.flatten(), test_geom_features))\n",
    "\n",
    "# Pad or truncate the test features to match the expected number of features\n",
    "expected_features = scaler1.n_features_in_\n",
    "test_feature_size = test_features.size\n",
    "if test_feature_size < expected_features:\n",
    "    test_features = np.pad(test_features, (0, expected_features - test_feature_size), mode='constant')\n",
    "elif test_feature_size > expected_features:\n",
    "    test_features = test_features[:expected_features]\n",
    "\n",
    "test_features = scaler1.transform(test_features.reshape(1, -1))  # Apply scaling from scaler1\n",
    "\n",
    "prediction = clf.predict(test_features)\n",
    "\n",
    "if prediction[0] == 1:\n",
    "    print(\"Genuine signature\")\n",
    "else:\n",
    "    print(\"Forged signature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from dtaidistance import dtw\n",
    "\n",
    "# Load and preprocess the images\n",
    "img1_path = r\"D:\\final_year_project\\data\\genuine\\005005_000.png\"\n",
    "img2_path = r\"D:\\final_year_project\\data\\genuine\\005005_001.png\"\n",
    "\n",
    "img1 = cv2.imread(img1_path, 0)\n",
    "img2 = cv2.imread(img2_path, 0)\n",
    "\n",
    "# Check if the images were loaded successfully\n",
    "if img1 is None or img2 is None:\n",
    "    print(\"Failed to load images.\")\n",
    "    exit()\n",
    "\n",
    "# Resize and normalize the images\n",
    "img1 = cv2.resize(img1, (800, 600))\n",
    "img2 = cv2.resize(img2, (800, 600))\n",
    "img1 = cv2.bitwise_not(img1)\n",
    "img2 = cv2.bitwise_not(img2)\n",
    "\n",
    "# Extract pen coordinates from the images\n",
    "def extract_pen_coordinates(image):\n",
    "    coords = np.where(image > 0)\n",
    "    return np.ravel(np.column_stack(coords))\n",
    "\n",
    "\n",
    "coords1 = extract_pen_coordinates(img1)\n",
    "coords2 = extract_pen_coordinates(img2)\n",
    "\n",
    "# Compute DTW distance between the signatures\n",
    "distance = dtw.distance(coords1, coords2)\n",
    "\n",
    "# Set a threshold for classifying as genuine or forged\n",
    "threshold = 50  # Adjust this value based on your data\n",
    "\n",
    "if distance < threshold:\n",
    "    print(\"Genuine signature\")\n",
    "else:\n",
    "    print(\"Forged signature\")\n",
    "\n",
    "# Optionally, you can visualize the warping path\n",
    "warping_path = dtw.warping_path(coords1, coords2)\n",
    "warped_coords1 = coords1[warping_path[0]]\n",
    "warped_coords2 = coords2[warping_path[1]]\n",
    "\n",
    "# Display the warped signatures (optional)\n",
    "warped_img1 = np.zeros_like(img1)\n",
    "warped_img2 = np.zeros_like(img2)\n",
    "\n",
    "for x, y in warped_coords1:\n",
    "    warped_img1[y, x] = 255\n",
    "\n",
    "for x, y in warped_coords2:\n",
    "    warped_img2[y, x] = 255\n",
    "\n",
    "cv2.imshow('Warped Signature 1', warped_img1)\n",
    "cv2.imshow('Warped Signature 2', warped_img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ccf0b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dtaidistance\n",
      "  Downloading dtaidistance-2.3.11-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: numpy in d:\\pythonliz\\lib\\site-packages (from dtaidistance) (1.21.5)\n",
      "Installing collected packages: dtaidistance\n",
      "Successfully installed dtaidistance-2.3.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dtaidistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975a32b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Matches: 65\n",
      "Percentage of match: 100.00%\n",
      "Length of good matches: 65\n",
      "Original\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "class FeatureMatching:\n",
    "    def __init__(self, image1_path, image2_path):\n",
    "        self.img1 = cv2.imread(image1_path)\n",
    "        self.img2 = cv2.imread(image2_path)\n",
    "\n",
    "    def preprocess_images(self):\n",
    "        self.img1 = cv2.cvtColor(self.img1, cv2.COLOR_BGR2GRAY)\n",
    "        self.img1 = cv2.resize(self.img1, (800, 600))\n",
    "        self.img2 = cv2.cvtColor(self.img2, cv2.COLOR_BGR2GRAY)\n",
    "        self.img2 = cv2.resize(self.img2, (800, 600))\n",
    "\n",
    "    def find_features(self):\n",
    "        self.sift = cv2.SIFT_create(nfeatures=500, nOctaveLayers=5, contrastThreshold=0.09, edgeThreshold=11, sigma=1.8)\n",
    "        self.kp1, self.des1 = self.sift.detectAndCompute(self.img1, None)\n",
    "        self.kp2, self.des2 = self.sift.detectAndCompute(self.img2, None)\n",
    "\n",
    "    def match_features(self):\n",
    "        bf = cv2.BFMatcher()\n",
    "        self.matches = bf.knnMatch(self.des1, self.des2, k=3)\n",
    "\n",
    "    def filter_matches(self):\n",
    "        self.good_matches = []\n",
    "        for match_list in self.matches:\n",
    "            m, n = match_list[:2]\n",
    "            if m.distance < 0.75 * n.distance:\n",
    "                self.good_matches.append([m])\n",
    "\n",
    "    def calculate_match_percentage(self):\n",
    "        self.match_percentage = len(self.good_matches) / len(self.kp1) * 100\n",
    "\n",
    "    def check_authenticity(self):\n",
    "        if self.match_percentage < 13.5:\n",
    "            print(\"Forged\")\n",
    "        else:\n",
    "            print(\"Original\")\n",
    "\n",
    "    def draw_matches(self):\n",
    "        img_matches = cv2.drawMatchesKnn(self.img1, self.kp1, self.img2, self.kp2, self.good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        cv2.imshow('Matches', img_matches)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image1_path = r\"D:\\final_year_project\\data\\genuine\\005005_000.png\"\n",
    "    image2_path = r\"D:\\final_year_project\\data\\genuine\\005005_000.png\"\n",
    "\n",
    "    feature_matching = FeatureMatching(image1_path, image2_path)\n",
    "    feature_matching.preprocess_images()\n",
    "    feature_matching.find_features()\n",
    "    feature_matching.match_features()\n",
    "    feature_matching.filter_matches()\n",
    "    feature_matching.calculate_match_percentage()\n",
    "    print(f\"Length of Matches: {len(feature_matching.matches)}\")\n",
    "    print(f\"Percentage of match: {feature_matching.match_percentage:.2f}%\")\n",
    "    print(f\"Length of good matches: {len(feature_matching.good_matches)}\")\n",
    "    feature_matching.check_authenticity()\n",
    "    feature_matching.draw_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82506ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
